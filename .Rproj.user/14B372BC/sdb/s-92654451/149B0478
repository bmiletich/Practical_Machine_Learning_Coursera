{
    "collab_server" : "",
    "contents" : "#START: Course Project\n\n#Objectives:\n#1. Describe how you built the model.\n\n#2. Describe how you used cross validation\n#I will split the training set into 5 training subsets with cross-validation set in my training control.\n\n#3. What do I think the out-of-sample error will be?\n# The out-of-sample error will be higher than my in-sample error rate due to the in-sample having only 6 participants.\n# The random forests methodology will be \n\nlibrary(caret)\nlibrary(ggplot2)\ntraining<-read.csv(file=\"pml-training.csv\")\n\n#dimensions\ndim(training)\n\n#examining properties\nstr(training)\ntraining.classes<-NULL\nfor(i in 1:dim(training)[2]){training.classes<-append(training.classes,class(training[,i]))}\n\n#features that are factors:\ntraining.factors<-names(training)[training.classes==\"factor\"]\nsum(as.numeric(training.classes==\"factor\"))\n#features that are numeric:\ntraining.numerics<-names(training)[training.classes==\"numeric\"]\nsum(as.numeric(training.classes==\"numeric\"))\n#features that are integer:\ntraining.integers<-names(training)[training.classes==\"integer\"]\nsum(as.numeric(training.classes==\"integer\"))\n\n\n#Are the numerics complete?\nnumericNAs<-NULL\n\nfor(i in 1:dim(training[,training.numerics])[2]){\n  pct.comp<-NULL\n  print(paste0(\"Numeric Feature: \",training.numerics[i]))\n  pct.comp<-sum(as.numeric(complete.cases(training[,training.numerics[i]])))/length(training[,1])\n  print(pct.comp)\n  numericNAs<-append(numericNAs,pct.comp)\n}\n#Since only 2% or so is only available for (non-NA) for several numeric features, I will be discarding features non-complete data.\ntraining.numerics.comp<-training.numerics[numericNAs==1]\n\n#Are the factors complete?\nfactorsNAs<-NULL\n\nfor(i in 1:dim(training[,training.factors])[2]){\n  pct.comp<-NULL\n  print(paste0(\"Factor Feature: \",training.factors[i]))\n  pct.comp<-sum(as.numeric(!training[,training.factors[i]]==\"\"))/dim(training)[1]\n  print(pct.comp)\n  factorsNAs<-append(factorsNAs,pct.comp)\n}\ntraining.factors.comp<-training.factors[factorsNAs==1]\n\n#Are the integers complete?\nintegersNAs<-NULL\n\nfor(i in 1:dim(training[,training.integers])[2]){\n  pct.comp<-NULL\n  print(paste0(\"Integer Feature: \",training.integers[i]))\n  pct.comp<-sum(as.numeric(complete.cases(training[,training.integers[i]])))/dim(training)[1]\n  print(pct.comp)\n  integersNAs<-append(integersNAs,pct.comp)\n}\n\n#Since only 2% or so is only available for (non-NA) for several integer features, I will be discarding features non-complete data.\n#It would not be reliable to attempt to impute from them.\ntraining.integers.comp<-training.integers[integersNAs==1]\n\n\nset.seed(\"5432\")\n\n#Concatenate the known good numeric/int\ntraining.quant<-append(training.integers.comp, training.numerics.comp)\n\n#concatenate the factors with the quant\ntraining.comp<-append(training.quant,training.factors.comp)\n\n#remove fields for timestamps and usernames\ntraining.features<-training.comp[-grep(\"^.*user|^.*time|^.*window|^X$\",training.comp)]\n\n\n#splitting training into two sets: training, and testing.\nset.seed(5432)\ninTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)\ntraining.sub<-training[inTrain,]\ntraining.sub<-training.sub[,c(training.features)]\ntesting.sub<-training[-inTrain,]\ntesting.sub<-testing.sub[,c(training.features)]\n\n#Create a decision tree of training based on rpart.\n#Need to shrink set by 1/2 to fit into memory\nset.seed(5432)\ntraining.sub.dtree<-training.sub[createDataPartition(training.sub$classe,p=0.5,list=FALSE),]\nset.seed(5432)\ndTree.modFit<-train(classe~.,method=\"rpart\",data=training.sub.dtree)\n\nlibrary(rattle)\n\nfancyRpartPlot(dTree.modFit$finalModel,shadow.offset=0)\n\ndTree.predict<-predict(dTree.modFit,testing.sub)\n\n#We can see that the decision tree is only around 48.97% accurate.\nconfusionMatrix(testing.sub$classe,dTree.predict)\n\n\n\n#Developing random forests on the training subset. Using the training subset with a 4-fold cross-validation\n#Using parallel rf and formatting predictors and outcomes.\nlibrary(mlbench)\nx=training.sub[,-53]\ny=training.sub[,53]\nlibrary(parallel)\nlibrary(doParallel)\ncluster <- makeCluster(detectCores() - 1) #leaves 1 core for OS\nregisterDoParallel(cluster)\n\n#Train RF model\nset.seed(5432)\ntrain_control<-trainControl(method=\"cv\",number=4,allowParallel=TRUE)\nrf.modFit<-train(x,y,method=\"rf\",trControl=train_control)\n\n#Shut down parallel cluster\nstopCluster(cluster)\nregisterDoSEQ()\n\n#Getting the variances for each variable:\nvarImp(rf.modFit)\n\n#Predict with random forests:\nrf.predict<-predict(rf.modFit,testing.sub)\n#we get a 99.08% accuracy with the random forests prediction model against out-of-sample test subset.\nconfusionMatrix(testing.sub$classe,rf.predict)\n\n\n#Predict the scores with the test data set of 20 entries.\n\ntestData<-read.csv(file=\"pml-testing.csv\")\ntestData.clean<-testData[,c(training.features[-53])]\nrf.predict.td<-predict(rf.modFit,testData.clean)\nrf.predict.td\n",
    "created" : 1481828785090.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4167225623",
    "id" : "149B0478",
    "lastKnownWriteTime" : 1486344369,
    "last_content_update" : 1486344369954,
    "path" : "C:/Users/Bryan/Google Drive/MBA Classes/Practical Machine Learning - Coursera/Course_Project/health_tracker_prediction_project/Project_Draft.R",
    "project_path" : "Project_Draft.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}