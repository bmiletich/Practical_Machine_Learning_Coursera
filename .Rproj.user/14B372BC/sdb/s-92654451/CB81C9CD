{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Practical_Machine_Learning_Human_Activity_Recognition\"\nauthor: \"Bryan Miletich\"\ndate: \"February 5, 2017\"\noutput: html_document\n---\n###Introduction:\n\nThis analysis was created to see if a suitable machine learning algorithm could be developed to accurately predict whether an individual is correctly performing an exercise.\n\nThe dataset was generated with a variety of motion tracking devices utilizing accelerometers, magnets, and other miscellaneous tools. The tracked regions are belts, forearms, arm, and dumbbell. The sample size of individuals included in this study is 6.\n\nwhat you think the expected out of sample error is, and why you made the choices you did.\n\n###Data Cleaning:\n\nThe data was cleaned by removing features with incomplete data.\nThe features with incomplete data tended to have only 2% available for the entire dataset, making it unreliable to perform data imputation.\n\nFurthermore, extraneous features regarding the username, timestamps, time windows, and a sequence number were removed.\n\n```{r, cache=FALSE, message=FALSE}\nlibrary(caret)\nlibrary(ggplot2)\ntraining<-read.csv(file=\"pml-training.csv\")\n#dimensions\n#str(training)\ndim(training)\ntraining.classes<-NULL\nfor(i in 1:dim(training)[2]){training.classes<-append(training.classes,class(training[,i]))}\n#features that are factors:\ntraining.factors<-names(training)[training.classes==\"factor\"]\nsum(as.numeric(training.classes==\"factor\"))\n#features that are numeric:\ntraining.numerics<-names(training)[training.classes==\"numeric\"]\nsum(as.numeric(training.classes==\"numeric\"))\n#features that are integer:\ntraining.integers<-names(training)[training.classes==\"integer\"]\nsum(as.numeric(training.classes==\"integer\"))\n\n#Are the numerics complete?\nnumericNAs<-NULL\nfor(i in 1:dim(training[,training.numerics])[2]){\n  pct.comp<-NULL\n  pct.comp<-sum(as.numeric(complete.cases(training[,training.numerics[i]])))/length(training[,1])\n  numericNAs<-append(numericNAs,pct.comp)\n}\nnumericNAs\ntraining.numerics.comp<-training.numerics[numericNAs==1]\n\n#Are the factors complete?\nfactorsNAs<-NULL\nfor(i in 1:dim(training[,training.factors])[2]){\n  pct.comp<-NULL\n  pct.comp<-sum(as.numeric(!training[,training.factors[i]]==\"\"))/dim(training)[1]\n  factorsNAs<-append(factorsNAs,pct.comp)\n}\nfactorsNAs\ntraining.factors.comp<-training.factors[factorsNAs==1]\n\n#Are the integers complete?\nintegersNAs<-NULL\nfor(i in 1:dim(training[,training.integers])[2]){\n  pct.comp<-NULL\n  pct.comp<-sum(as.numeric(complete.cases(training[,training.integers[i]])))/dim(training)[1]\n  integersNAs<-append(integersNAs,pct.comp)\n}\nintegersNAs\ntraining.integers.comp<-training.integers[integersNAs==1]\n\nset.seed(\"5432\")\n#Concatenate known good features.\ntraining.quant<-append(training.integers.comp, training.numerics.comp)\ntraining.comp<-append(training.quant,training.factors.comp)\n#remove fields for timestamps, time windows, usernames, and the X seq number column\ntraining.features<-training.comp[-grep(\"^.*user|^.*time|^.*window|^X$\",training.comp)]\n\n#splitting training into two sets: training, and testing.\nset.seed(5432)\ninTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)\ntraining.sub<-training[inTrain,]\ntraining.sub<-training.sub[,c(training.features)]\ntesting.sub<-training[-inTrain,]\ntesting.sub<-testing.sub[,c(training.features)]\n```\n###Model Building Attempts:\n\nTwo models were attempted in this analysis.\n\nThe first model was a decision tree, and was only 48.97% accurate:\n```{r, cache=FALSE, message=FALSE}\n#Create a decision tree of training based on rpart.\n#Need to shrink set by 1/2 to fit into memory\nset.seed(5432)\ntraining.sub.dtree<-training.sub[createDataPartition(training.sub$classe,p=0.5,list=FALSE),]\nset.seed(5432)\ndTree.modFit<-train(classe~.,method=\"rpart\",data=training.sub.dtree)\n\nlibrary(rattle)\n\nfancyRpartPlot(dTree.modFit$finalModel,shadow.offset=0)\n\ndTree.predict<-predict(dTree.modFit,testing.sub)\n\n#We can see that the decision tree is only around 48.97% accurate.\nconfusionMatrix(testing.sub$classe,dTree.predict)\n```\n\n\nThe second model was with random forests with 4-fold cross validation specified in the train function. The \"parallel\" and \"doParallel\" libraries were necessary to have the random forest training model complete in a timely manner.\n```{r, cache=FALSE, message=FALSE}\n#Using parallel rf and formatting predictors and outcomes.\nlibrary(mlbench)\nx=training.sub[,-53]\ny=training.sub[,53]\nlibrary(parallel)\nlibrary(doParallel)\n\ncluster <- makeCluster(detectCores() - 1) #leaves 1 core for OS\nregisterDoParallel(cluster)\n#Train RF model\nset.seed(5432)\ntrain_control<-trainControl(method=\"cv\",number=4,allowParallel=TRUE)\nrf.modFit<-train(x,y,method=\"rf\",trControl=train_control)\n#Shut down parallel cluster\nstopCluster(cluster)\nregisterDoSEQ()\n\n#Getting the variances for each variable:\nvarImp(rf.modFit)\n```\n###In Sample Error and Out of Sample Error:\nThe in-sample error for the random forests model was 100% accurate, which is a strong sign that this model is overfitting the data.\n```{r, cache=FALSE, message=FALSE}\nrf.predict.in<-predict(rf.modFit,training.sub)\nconfusionMatrix(training.sub$classe,rf.predict.in)\n```\nThe out-of-sample error was shown to be 98.08% accurate. This was with the second training subset (labeled testing.sub):\n```{r, cache=FALSE, message=FALSE}\n#Predict with random forests:\nrf.predict<-predict(rf.modFit,testing.sub)\n#we get a 99.08% accuracy with the random forests prediction model against out-of-sample test subset.\nconfusionMatrix(testing.sub$classe,rf.predict)\n```\nThis study only had 6 participants in its design, so I would expect the model to be somewhat more inaccurate when used outside this sample (leading to higher than measured out-of-sample errors) given the small study sample size.\n\nI am using the random forest model for predicting against the test data set below.\n\n###Predicting the test data set.\n\nThese are the predicted classifications with the test data set of 20 entries:\n```{r, cache=FALSE, message=FALSE}\ntestData<-read.csv(file=\"pml-testing.csv\")\n#clean test dataframe\ntestData.clean<-testData[,c(training.features[-53])]\nrf.predict.td<-predict(rf.modFit,testData.clean)\nrf.predict.td\n```\n\nThe prediction accuracy with the random forests model was 100% against this test data set.",
    "created" : 1486343618285.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3042467304",
    "id" : "CB81C9CD",
    "lastKnownWriteTime" : 1486347257,
    "last_content_update" : 1486347257517,
    "path" : "C:/Users/Bryan/Google Drive/MBA Classes/Practical Machine Learning - Coursera/Course_Project/health_tracker_prediction_project/Practical_Machine_Learning_Human_Activity_Recognition_Writeup.Rmd",
    "project_path" : "Practical_Machine_Learning_Human_Activity_Recognition_Writeup.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}