getwd()
dir()
#START: Course Project
#Objectives:
#1. Describe how you built the model.
#2. Describe how you used cross validation
#I will split the training set into 9 training subsets and 1 testing subset with k-fold validation.
#3. What do I think the out-of-sample error will be?
#Depends on bias.
library(caret)
library(ggplot2)
training<-read.csv(file="pml-training.csv")
#dimensions
dim(training)
#examining properties
str(training)
training.classes<-NULL
for(i in 1:dim(training)[2]){training.classes<-append(training.classes,class(training[,i]))}
#features that are factors:
training.factors<-names(training)[training.classes=="factor"]
sum(as.numeric(training.classes=="factor"))
#features that are numeric:
training.numerics<-names(training)[training.classes=="numeric"]
sum(as.numeric(training.classes=="numeric"))
#features that are integer:
training.integers<-names(training)[training.classes=="integer"]
sum(as.numeric(training.classes=="integer"))
#Are the numerics complete?
numericNAs<-NULL
for(i in 1:dim(training[,training.numerics])[2]){
pct.comp<-NULL
print(paste0("Numeric Feature: ",training.numerics[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.numerics[i]])))/length(training[,1])
print(pct.comp)
numericNAs<-append(numericNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several numeric features, I will be discarding those features.
training.numerics.comp<-training.numerics[numericNAs>0.9]
#Are the factors complete?
factorsNAs<-NULL
for(i in 1:dim(training[,training.factors])[2]){
pct.comp<-NULL
print(paste0("Factor Feature: ",training.factors[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.factors[i]])))/length(training[,1])
print(pct.comp)
factorsNAs<-append(factorsNAs,pct.comp)
}
#Since all factors are available, I will include them in the preliminary analysis.
training.factors.comp<-training.factors
#Are the integers complete?
integersNAs<-NULL
for(i in 1:dim(training[,training.integers])[2]){
pct.comp<-NULL
print(paste0("Integer Feature: ",training.integers[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.integers[i]])))/length(training[,1])
print(pct.comp)
integersNAs<-append(integersNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several integer features, I will be discarding those features with low completion rates.
#It would not be reliable to attempt to impute from them.
training.integers.comp<-training.integers[integersNAs>0.9]
set.seed("123456789")
training.quant<-append(training.integers.comp, training.numerics.comp)
training.quant<-append(training.quant,c("classe"))
#concatenate the factors with the quant
training.comp<-append(training.quant,training.factors.comp)
decTree.modFit<-train(Classe~.,method="rpart",data=training.sub)
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
training.sub<-training[inTrain,]
testing.sub<-training[-inTrain,]
names(training.sub)
glob2rx("*belt*")
grep("^.*belt|^.*arm|^.*dumbell",names(training.sub),value=TRUE)
trainin.comp
training.comp
grep("^.*belt|^.*arm|^.*dumbell",names(training.comp),value=TRUE)
grep("^.*belt|^.*arm|^.*dumbell",training.comp,value=TRUE)
training.features<-grep("^.*belt|^.*arm|^.*dumbell",training.comp,value=TRUE)
training.sub<-training[inTrain,select=training.features]
training.sub<-training[inTrain,select=c(training.features)]
training.sub<-training[training.features,]
names(training.sub)
training.features
training.sub<-training[select=c(training.features)]
training.sub<-training[select=c(training.features),]
training.sub<-training[c,(training.features)]
training.sub<-training[,c(training.features)]
names(training.sub)
training.sub<-training[,c(training.features)]
testing.sub<-training[-inTrain,]
testing.sub<-testing.sub[,c(training.features)]
source('C:/Users/Bryan/Google Drive/MBA Classes/Practical Machine Learning - Coursera/Course_Project/health_tracker_prediction_project/Project_Draft.R')
decTree.modFit<-train(Classe~.,method="rpart",data=training.sub)
names(training.sub)
names(training)
class(training$classe[1])
training.features<-grep("^.*belt|^.*arm|^.*dumbell|classe",training.comp,value=TRUE)
source('C:/Users/Bryan/Google Drive/MBA Classes/Practical Machine Learning - Coursera/Course_Project/health_tracker_prediction_project/Project_Draft.R')
training.features<-grep("^.*belt|^.*arm|^.*dumbell|classe",training.comp,value=TRUE)
training.sub<-training[,c(training.features)]
testing.sub<-testing.sub[,c(training.features)]
names(training.sub)
names(training.sub)
names(training)
training.comp
grep("^*.classe",names(training.comp))
grep("^.*classe",names(training.comp))
grep("^*.lasse",names(training.comp))
grep("^.*lasse",names(training.comp))
glob2rx("*classe*")
grep("^.*classe",names(training.comp))
names(training.comp)
training.comp<-append(training.quant,training.factors.comp)
training.comp
grep("^.*classe",names(training.comp))
training.comp
grep("^.*classe",training.comp)
grep("^.*classe",training.comp,values=TRUE)
grep("^.*classe",training.comp,value=TRUE)
training.comp<-append(training.quant,training.factors.comp)
training.features<-grep("^.*belt|^.*arm|^.*dumbell|classe",training.comp,value=TRUE)
training.features
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
training.sub<-training[inTrain,]
training.sub<-training[,c(training.features)]
testing.sub<-training[-inTrain,]
testing.sub<-testing.sub[,c(training.features)]
decTree.modFit<-train(classe~.,method="rpart",data=training.sub)
rm(training)
decTree.modFit<-train(classe~.,method="rpart",data=training.sub)
library(caret)
library(ggplot2)
decTree.modFit<-train(classe~.,method="rpart",data=training.sub)
warnings()
rm(testing.sub)
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
training.sub<-training[inTrain,]
training.sub<-training.sub[,c(training.features)]
training<-read.csv(file="pml-training.csv")
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
training.sub<-training[inTrain,]
training.sub<-training.sub[,c(training.features)]
library(caret)
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
training.sub<-training[inTrain,]
training.sub<-training.sub[,c(training.features)]
decTree.modFit<-train(classe~.,method="rpart",data=training.sub)
dim(training.sub)
dim(training)
rm(training)
library(caret)
training.sub.dtree<-training.sub[createDataPartition(training.sub$classe,p=0.5,list=FALSE),]
decTree.modFit<-train(classe~.,method="rpart",data=training.sub.dtree,model=FALSE)
library(rattle)
print(decTree.modFit)
print(decTree.modFit$finalModel)
print(decTree.modFit$finalModel)
fancyRpartPlot(decTree.modFit$finalModel,cex=.5,under.cex=1,shadow.offset=0)
decTree.modFit<-train(classe~.,method="rpart",data=training.sub.dtree)
fancyRpartPlot(decTree.modFit$finalModel,cex=.5,under.cex=1,shadow.offset=0)
fancyRpartPlot(decTree.modFit$finalModel,shadow.offset=0)
decTree.modFit$finalModel
print(decTree.modFit$finalModel)
training.sub.dtree
names(training.sub.dtree)
training.features
training<-read.csv(file="pml-training.csv")
names(training)
grep("^.*classe",names(training))
grep("^.*Classe",names(training))
names(training.quant)
#START: Course Project
#Objectives:
#1. Describe how you built the model.
#2. Describe how you used cross validation
#I will split the training set into 9 training subsets and 1 testing subset with k-fold validation.
#3. What do I think the out-of-sample error will be?
#Depends on bias.
library(caret)
library(ggplot2)
training<-read.csv(file="pml-training.csv")
#dimensions
dim(training)
#examining properties
str(training)
training.classes<-NULL
for(i in 1:dim(training)[2]){training.classes<-append(training.classes,class(training[,i]))}
#features that are factors:
training.factors<-names(training)[training.classes=="factor"]
sum(as.numeric(training.classes=="factor"))
#features that are numeric:
training.numerics<-names(training)[training.classes=="numeric"]
sum(as.numeric(training.classes=="numeric"))
#features that are integer:
training.integers<-names(training)[training.classes=="integer"]
sum(as.numeric(training.classes=="integer"))
#Are the numerics complete?
numericNAs<-NULL
for(i in 1:dim(training[,training.numerics])[2]){
pct.comp<-NULL
print(paste0("Numeric Feature: ",training.numerics[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.numerics[i]])))/length(training[,1])
print(pct.comp)
numericNAs<-append(numericNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several numeric features, I will be discarding those features.
training.numerics.comp<-training.numerics[numericNAs>0.9]
#Are the factors complete?
factorsNAs<-NULL
for(i in 1:dim(training[,training.factors])[2]){
pct.comp<-NULL
print(paste0("Factor Feature: ",training.factors[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.factors[i]])))/length(training[,1])
print(pct.comp)
factorsNAs<-append(factorsNAs,pct.comp)
}
#Since all factors are available, I will include them in the preliminary analysis.
training.factors.comp<-training.factors
#Are the integers complete?
integersNAs<-NULL
for(i in 1:dim(training[,training.integers])[2]){
pct.comp<-NULL
print(paste0("Integer Feature: ",training.integers[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.integers[i]])))/length(training[,1])
print(pct.comp)
integersNAs<-append(integersNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several integer features, I will be discarding those features with low completion rates.
#It would not be reliable to attempt to impute from them.
training.integers.comp<-training.integers[integersNAs>0.9]
set.seed("123456789")
#Concatenate the known good numeric/int
training.quant<-append(training.integers.comp, training.numerics.comp)
names(training.quant)
cat(training.quant)
training.quant
grep("^.*classe"),training.quant)
grep("^.*classe"),training.quant)
training.factors.comp
library(caret)
library(ggplot2)
training<-read.csv(file="pml-training.csv")
#dimensions
dim(training)
#examining properties
str(training)
training.classes<-NULL
for(i in 1:dim(training)[2]){training.classes<-append(training.classes,class(training[,i]))}
#features that are factors:
training.factors<-names(training)[training.classes=="factor"]
sum(as.numeric(training.classes=="factor"))
#features that are numeric:
training.numerics<-names(training)[training.classes=="numeric"]
sum(as.numeric(training.classes=="numeric"))
#features that are integer:
training.integers<-names(training)[training.classes=="integer"]
sum(as.numeric(training.classes=="integer"))
#Are the numerics complete?
numericNAs<-NULL
for(i in 1:dim(training[,training.numerics])[2]){
pct.comp<-NULL
print(paste0("Numeric Feature: ",training.numerics[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.numerics[i]])))/length(training[,1])
print(pct.comp)
numericNAs<-append(numericNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several numeric features, I will be discarding those features.
training.numerics.comp<-training.numerics[numericNAs>0.9]
#Are the factors complete?
factorsNAs<-NULL
for(i in 1:dim(training[,training.factors])[2]){
pct.comp<-NULL
print(paste0("Factor Feature: ",training.factors[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.factors[i]])))/length(training[,1])
print(pct.comp)
factorsNAs<-append(factorsNAs,pct.comp)
}
#Since all factors are available, I will include them in the preliminary analysis.
training.factors.comp<-training.factors
#Are the integers complete?
integersNAs<-NULL
for(i in 1:dim(training[,training.integers])[2]){
pct.comp<-NULL
print(paste0("Integer Feature: ",training.integers[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.integers[i]])))/length(training[,1])
print(pct.comp)
integersNAs<-append(integersNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several integer features, I will be discarding those features with low completion rates.
#It would not be reliable to attempt to impute from them.
training.integers.comp<-training.integers[integersNAs>0.9]
set.seed("123456789")
#Concatenate the known good numeric/int
training.quant<-append(training.integers.comp, training.numerics.comp)
#concatenate the factors with the quant
training.comp<-append(training.quant,training.factors.comp)
#search for relevant fields for forearms, arms, belt, and dumbbells from the features that are complete.
training.features<-grep("^.*belt|^.*arm|^.*dumbell|classe",training.comp,value=TRUE)
#splitting training into two sets: training, and testing.
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
training.sub<-training[inTrain,]
training.sub<-training.sub[,c(training.features)]
testing.sub<-training[-inTrain,]
testing.sub<-testing.sub[,c(training.features)]
class(training.sub)
names(training.sub)
training.sub.dtree<-training.sub[createDataPartition(training.sub$classe,p=0.5,list=FALSE),]
decTree.modFit<-train(classe~.,method="rpart",data=training.sub.dtree)
print(decTree.modFit$finalModel)
fancyRpartPlot(decTree.modFit$finalModel,shadow.offset=0)
decTree.predict<-predict(decTree.modFit,testing.sub)
confusionMatrix(testing.sub$Classe,decTree.predict)
names(testing.sub)
names(decTree.predict)
testing.sub$classe
confusionMatrix(testing.sub$classe,decTree.predict)
rf.modFit<-train(classe~.,data=training.sub,method="rf",prox=TRUE)
train_control<-trainControl(method="cv",number=5)
rf.modFit<-train(classe~.,data=training.sub,method="rf",prox=TRUE,trControl=train_control)
training.sub$classe
unique(training.sub$classe)
?train_control
?trainControl
train_control<-trainControl(method="cv",number=5,allowParallel=TRUE)
rf.modFit<-train(classe~.,data=training.sub,method="rf",prox=TRUE,trControl=train_control)
training.sub[-c(classe),]
training.sub[-c("classe"),]
training.sub[,-c("classe")]
?train
training.sub[,-c("classe")]
class(training.sub)
training.sub[-c("classe")]
training.sub[-c("classe"),]
training.sub[,-c("classe")]
training.sub[,-c("classe"),]
training.features
training.features[-c(classe)]
training.features[-c("classe")]
training.sub
training.sub$classe
names(training.sub)
training.sub[,-64]
names(training.sub[,-64])
rf.modFit<-train(y=training.sub$classe,x=training.sub[,-64],method="rf",prox=TRUE,trControl=train_control)
warnings()
names(training.sub)
grep("^.*magnet",names(training.sub))
length(grep("^.*magnet",names(training.sub)))
training.features1<-training.features[,-grep("^.*magnet",training.features)]
training.features1<-training.features[,-(grep("^.*magnet",training.features))]
training.features
training.features1<-training.features[,-(grep("^.*magnet",training.features))]
grep("^.*magnet",training.features)
-grep("^.*magnet",training.features)
grep("^.*magnet",training.features)
training.features[,grep("^.*magnet",training.features)]
training.features[,unlist(grep("^.*magnet",training.features))]
training.features[-(grep("^.*magnet",training.features))]
training.features[-(grep("^.*magnet",training.features))]
glob2rx("*magnet*")
training.features[-(grep("^.*magnet",training.features))]
warnings()
str(training.sub)
names(str(training.sub))
grep("Factor",str(training.sub))
grep("Factor",unlist(str(training.sub)))
training.factors<-names(training)[training.classes=="factor"]
length(training.factors)
training.sub$classe
training.features[-(grep("^.*magnet",training.features))]
preObj <- preProcess(training.sub[, -64], method=c("center", "scale", "pca"), thresh=0.9)
preObj
preObj <- preProcess(training.sub[, -64], method=c("center", "scale", "pca"), thresh=0.8)
preObj
preObj <- preProcess(training.sub[, -64], method=c("center", "scale", "pca"), thresh=0.99)
preObj
preObj <- preProcess(training.sub[, -64], method=c("center", "scale", "pca"), thresh=0.999)
preObj
summary(preObj)
preObj$pcaComp
preObj$data
preObj$numComp
preObj
?preProcess
varImp()
?varImp
rf.modFit<-train(y=training.sub$classe,x=training.sub[,-64],method="rf",prox=TRUE,trControl=train_control)
rf.modFit<-train(training.sub$classe~training.sub[,-64],method="rf",prox=TRUE,trControl=train_control)
rf.modFit<-train(training.sub$classe~.,data=training.sub,method="rf",prox=TRUE,trControl=train_control)
warnings()
help(memory.size)
memory.size()
memory.limit()
ls()
?rm
rm(list=ls())
#START: Course Project
#Objectives:
#1. Describe how you built the model.
#2. Describe how you used cross validation
#I will split the training set into 9 training subsets and 1 testing subset with k-fold validation.
#3. What do I think the out-of-sample error will be?
#Depends on bias.
library(caret)
library(ggplot2)
training<-read.csv(file="pml-training.csv")
#dimensions
dim(training)
#examining properties
str(training)
training.classes<-NULL
for(i in 1:dim(training)[2]){training.classes<-append(training.classes,class(training[,i]))}
#features that are factors:
training.factors<-names(training)[training.classes=="factor"]
sum(as.numeric(training.classes=="factor"))
#features that are numeric:
training.numerics<-names(training)[training.classes=="numeric"]
sum(as.numeric(training.classes=="numeric"))
#features that are integer:
training.integers<-names(training)[training.classes=="integer"]
sum(as.numeric(training.classes=="integer"))
#Are the numerics complete?
numericNAs<-NULL
for(i in 1:dim(training[,training.numerics])[2]){
pct.comp<-NULL
print(paste0("Numeric Feature: ",training.numerics[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.numerics[i]])))/length(training[,1])
print(pct.comp)
numericNAs<-append(numericNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several numeric features, I will be discarding those features.
training.numerics.comp<-training.numerics[numericNAs>0.9]
#Are the factors complete?
factorsNAs<-NULL
for(i in 1:dim(training[,training.factors])[2]){
pct.comp<-NULL
print(paste0("Factor Feature: ",training.factors[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.factors[i]])))/length(training[,1])
print(pct.comp)
factorsNAs<-append(factorsNAs,pct.comp)
}
#Since all factors are available, I will include them in the preliminary analysis.
training.factors.comp<-training.factors
#Are the integers complete?
integersNAs<-NULL
for(i in 1:dim(training[,training.integers])[2]){
pct.comp<-NULL
print(paste0("Integer Feature: ",training.integers[i]))
pct.comp<-sum(as.numeric(complete.cases(training[,training.integers[i]])))/length(training[,1])
print(pct.comp)
integersNAs<-append(integersNAs,pct.comp)
}
#Since only 2% or so is only available for (non-NA) for several integer features, I will be discarding those features with low completion rates.
#It would not be reliable to attempt to impute from them.
training.integers.comp<-training.integers[integersNAs>0.9]
set.seed("123456789")
#Concatenate the known good numeric/int
training.quant<-append(training.integers.comp, training.numerics.comp)
#concatenate the factors with the quant
training.comp<-append(training.quant,training.factors.comp)
#search for relevant fields for forearms, arms, belt, and dumbbells from the features that are complete.
training.features<-grep("^.*belt|^.*arm|^.*dumbell|classe",training.comp,value=TRUE)
#remove fields for magnets and amplitudes
training.features<-training.features[-grep("^.*magnet|^.*amplitude",training.features)]
#splitting training into two sets: training, and testing.
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
training.sub<-training[inTrain,]
training.sub<-training.sub[,c(training.features)]
training.sub.dtree<-training.sub[createDataPartition(training.sub$classe,p=0.5,list=FALSE),]
training.sub.dtree
names(training.sub.dtree)
decTree.modFit<-train(classe~.,method="rpart",data=training.sub.dtree)
library(rattle)
fancyRpartPlot(dTree.modFit$finalModel,shadow.offset=0)
dTree.predict<-predict(dTree.modFit,testing.sub)
library(rattle)
fancyRpartPlot(decTree.modFit$finalModel,shadow.offset=0)
decTree.predict<-predict(decTree.modFit,testing.sub)
testing.sub<-training[-inTrain,]
testing.sub<-testing.sub[,c(training.features)]
decTree.predict<-predict(decTree.modFit,testing.sub)
decTree.predict
confusionMatrix(testing.sub$classe,decTree.predict)
na,mes(testing.sub)
names(testing.sub)
rm(list=ls())
